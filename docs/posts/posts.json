[
  {
    "path": "posts/2021-02-24-textanalysis/",
    "title": "Text analysis",
    "description": "A brief example demonstrating how to conduct text analysis in R using tidytext--for fun I compare the most recent manuscripts from my M.S. and PhD advisers' labs.",
    "author": [
      {
        "name": "Julianna Renzi",
        "url": {}
      }
    ],
    "date": "2021-02-24",
    "categories": [],
    "contents": "\n\nContents\nPhD adviser: Deron Burkepile\nPrepare the data\nVisualize results\nConduct a sentiment analysis\n\nM.S. adviser: Brian Silliman\nPrepare the data\nVisualize results\nConduct a sentiment analysis\n\n\nIn this script I compare the words used in the most recent peer-reviewed paper by my PhD and Master’s adviser, where they were the senior (i.e. last) author. Although the lexicons and tools here are primarily for non-academic texts, this is a fun exercise to demonstrate what we can do with words in R. To see the code I used in each section click on the Show code option.\nPhD adviser: Deron Burkepile\nRead in my PhD adviser’s latest relevant publication, which was led by Dr. Leïla Ezzat.\nCitation: - Ezzat, L., Lamy, T., Maher, R.L., Munsterman, K.S., Landfield, K.M., Schmeltzer, E.R., Clements, C.S., Thurber, R.L.V. and Burkepile, D.E., 2020. Parrotfish predation drives distinct microbial communities in reef-building corals. Animal Microbiome, 2(1), p.5.\nPrepare the data\n\n\nShow code\n\nezzat_text <- pdf_text(here(\"_posts\", \"2021-02-24-textanalysis\", \"data\", \"Ezzat_et_al_2020.pdf\"))\n\n\n\nFirst, split the text up by line and trim excess white space\n\n\nShow code\n\nezzat_tidy <- data.frame(ezzat_text) %>% # now each row is a different page \n  mutate(text_full = str_split(ezzat_text, pattern = \"\\\\n\")) %>% # first slash just says look for \\n as a string\n# break it up using string split breaking wherever there is a line break\n# now each line is an element -- then want each element\n  unnest(text_full) %>% # now see repeated information but each line has it's own line\n  mutate(text_full = str_trim(text_full)) %>% # get rid of excess white spaces\n  slice(-1:-11) %>% # get rid of front matter/authors\n  slice(-25:-37) %>% # remove license etc.\n  slice(-545:-n()) %>% # remove supplement and citations\n  slice(-1:-24) # also post-facto decide to remove abstract/just focus on main text\n\n\n\nThen break up the PDF into logical sections for analysis\n\n\nShow code\n\n# break up the manuscript by traditional sections\nezzat_df <- ezzat_tidy %>% \n  mutate(section = ifelse(str_detect(text_full, pattern = \"Background\"), \"Introduction\",\n                  ifelse(str_detect(text_full, pattern = \"Results\"), \"Results\",\n                  ifelse(str_detect(text_full, pattern = \"Conclusion\"), \"Conclusion\",\n                  ifelse(str_detect(text_full, pattern = \"Material and methods\"), \"Methods\",\n                          \n                          NA_character_))))) %>% \n  fill(section) # fills in the NAs with the value above\n  # need to know this is in order to use fill()\n\n\n\nRemove numbers, citations, and figure/table references. Then get the data into tokenized text format, where one token is one single word using tidytext.\n\n\nShow code\n\nezzat_tokens <- ezzat_df %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"[0-9]+|[[:punct:]]|\\\\(.*\\\\)\", replacement = \"\")) %>% # first get rid of numbers and citations because we are only analyzing the text\n  # then remove Fig and Table labels\n  mutate(text_full = gsub(x = text_full, pattern = \"Fig.\", replacement = \"\")) %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"Tables\", replacement = \"\")) %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"Table\", replacement = \"\")) %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"Figure\", replacement = \"\")) %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"Figures\", replacement = \"\")) %>%\n  \n  unnest_tokens(word, text_full) %>% # from tidytext\n  select(-ezzat_text) # get rid of first column that holds no new information\n\n\n\nNow remove stop words (i.e. common words like “the”, “is”, and “it”)\n\n\nShow code\n\nezzat_nonstop_words <- ezzat_tokens %>% \n  anti_join(stop_words) # knows to un-join by matching column name\n# use ?stop_words to look at different stop_words lexicons\n\n\n# count them by section (this is equivalent to group_by + summarize)\nnonstop_counts <- ezzat_nonstop_words %>% \n  count(section, word)\n\n# find the top 10 words by section\ntop_10_words <- nonstop_counts %>% \n  group_by(section) %>% \n  arrange(-n) %>% \n  slice(1:10) # keep top ten\n\ntop_10_words\n\n\n# A tibble: 40 x 3\n# Groups:   section [4]\n   section    word            n\n   <chr>      <chr>       <int>\n 1 Conclusion coral          11\n 2 Conclusion corals          9\n 3 Conclusion parrotfish      8\n 4 Conclusion fish            4\n 5 Conclusion lobata          4\n 6 Conclusion microbiomes     4\n 7 Conclusion collected       3\n 8 Conclusion colonies        3\n 9 Conclusion colony          3\n10 Conclusion corallivory     3\n# … with 30 more rows\n\nVisualize results\nVisualize the top words\n\n\nShow code\n\nggplot(data = top_10_words, aes(x = reorder(word, n), y = n)) +\n  geom_col(fill = \"coral\") +\n  facet_wrap(~section, scales = \"free\") + # need scales = \"free\" to make it so axes (incl. x axis) is not the same in each plot\n  coord_flip() +\n  ylab(\"Word\") +\n  xlab(\"Number of times used\") +\n  theme_minimal()\n\n\n\n\nMake word clouds of the top 50 words in each section\n\n\nShow code\n\nintro_top50 <- \n  nonstop_counts %>% \n  filter(section == \"Introduction\") %>% \n  arrange(-n) %>% \n  slice(1:50)\n\nintro_cloud_Der <- ggplot(data = intro_top50, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n)) +\n  scale_size_area(max_size = 5.5) +\n  ggtitle('Burkepile') +\n  theme_void()\n\n# see some words got cut off, but we'll leave those be for now\n\n\n\n\n\nShow code\n\n# can make one for methods\nmethods_top50 <- \n  nonstop_counts %>% \n  filter(section == \"Methods\") %>% \n  arrange(-n) %>% \n  slice(1:50)\n\nmeth_cloud_Der <- ggplot(data = methods_top50, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n)) +\n  scale_colour_continuous(type = \"viridis\") +\n  scale_size_area(max_size = 5.5) +\n  theme_void() +\n  ggtitle('Burkepile')\n\n\n\n# or one for results\nresults_top50 <- \n  nonstop_counts %>% \n  filter(section == \"Results\") %>% \n  arrange(-n) %>% \n  slice(1:50)\n\nres_cloud_Der <- ggplot(data = results_top50, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n)) +\n  scale_colour_continuous(type = \"gradient\", low = \"grey\", high = \"coral\") +\n  scale_size_area(max_size = 5.5) +\n  theme_void() +\n  ggtitle('Burkepile')\n\n\n# or for the conclusion\nconcl_top50 <- \n  nonstop_counts %>% \n  filter(section == \"Conclusion\") %>% \n  arrange(-n) %>% \n  slice(1:50)\n\nconcl_cloud_Der <- ggplot(data = concl_top50, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n)) +\n  scale_colour_continuous(type = \"gradient\", low = \"orange\", high = \"darkorchid\") +\n  scale_size_area(max_size = 5.5) +\n  theme_void() +\n  ggtitle('Burkepile')\n\n\n\nConduct a sentiment analysis\nAlthough this is not as relevant for academic papers like Ezzat et al., I’ll conduct a sentiment analysis for fun where we look at whether the words used have positive or negative connotations. This will be biased given that Ezzat et al. is talking about predation/wounding (and given that the lexicons are not meant for this kind of writing), but is a fun exercise.\nI’ll use the afinn lexicon that ranks words on a scale of -5 (very negative) to 5 (very positive). We’ll only keep words that have a counterpart in the afinn lexicon, which will dramatically change the words we analyze (again, this is better for other forms of text, but I thought it would be interesting anyways).\n\n\nShow code\n\nafinn_lex <- get_sentiments(\"afinn\")\n\n# join matching words\nezzat_afinn <- ezzat_nonstop_words %>% \n  inner_join(get_sentiments(\"afinn\")) \n\n# can get total counts\nafinn_counts <- ezzat_afinn %>% \n  count(section, value) # see how positive and negative the values are for each section\n\n# or could get mean value\nafinn_means <- ezzat_afinn %>% \n  group_by(section) %>% \n  summarize(mean_afinn = mean(value))\n\n\n\nVisualize results\n\n\nShow code\n\nafinn_Der <- ggplot(data = afinn_means, aes(x = reorder(section, mean_afinn), y = mean_afinn)) +\n  geom_col(fill = \"coral\") +\n  ylab(\"Mean afinn value (-5 to 5)\") +\n  xlab(\"Paper section\") +\n  coord_flip() +\n  scale_y_continuous(limits = c(-1,1)) +\n  theme_bw() +\n  geom_vline(aes(xintercept = 0)) + \n  ggtitle(\"Burkepile\")\n\n\n\nOr we could use a different lexicon, like the NRC lexicon.\n\n\nShow code\n\nezzat_nrc <- ezzat_nonstop_words %>% \n  inner_join(get_sentiments(\"nrc\")) # have repeated values when there are multiple sentiments for a word\n\nezzat_nrc_counts <- ezzat_nrc %>% \n  count(section, sentiment) # 10 sentiments total in nrc\n\nnrc_Der <- ezzat_nrc_counts %>% \n  ggplot(aes(x = reorder(sentiment, n), y = n)) +\n  geom_col(fill = \"coral\") +\n  facet_wrap(~section) +\n  coord_flip() +\n  xlab(\"Seniment\") +\n  ylab(\"Word count\") +\n  theme_minimal() +\n  ggtitle(\"Burkepile\") # these are only words that have a value in the nrc lexicon\n\n\n\nM.S. adviser: Brian Silliman\nRead in my M.S. adviser’s latest relevant publication, which was led by Dr. Qiang He.\nCitation: - He, Q., Li, H., Xu, C., Sun, Q., Bertness, M.D., Fang, C., Li, B. and Silliman, B.R., 2020. Consumer regulation of the carbon cycle in coastal wetland ecosystems. Philosophical Transactions of the Royal Society B, 375(1814), p.20190451.\nPrepare the data\n\n\nShow code\n\nhe_text <- pdf_text(here(\"_posts\", \"2021-02-24-textanalysis\", \"data\", \"He_et_al_2020.pdf\"))\n\n\n\nFirst, split the text up by line and trim excess white space\n\n\nShow code\n\nhe_tidy <- data.frame(he_text) %>% \n  mutate(text_full = str_split(he_text, pattern = \"\\\\n\")) %>% \n  unnest(text_full) %>% \n  mutate(text_full = str_trim(text_full)) %>% # get rid of excess white spaces\n  slice(-1:-43) %>% # get rid of front matter/authors/abstract\n  slice(-500:-n()) # remove citations, etc.\n\n\n\nThen break up the PDF into logical sections for analysis\n\n\nShow code\n\n# break up the manuscript by traditional sections\n# these are slightly different than Ezzat et al, but we'll keep the names consistent\nhe_df <- he_tidy %>% \n  mutate(section = ifelse(str_detect(text_full, pattern = \"1. Introduction\"), \"Introduction\",\n                  ifelse(str_detect(text_full, pattern = \"2. Materials and methods\"), \"Methods\",\n                  ifelse(str_detect(text_full, pattern = \"3. Results\"), \"Results\",\n                  ifelse(str_detect(text_full, pattern = \"4. Discussion\"), \"Conclusion\",\n                          \n                          NA_character_))))) %>% \n  fill(section) \n\n\n\nRemove numbers, citations, and figure/table references. Then get the data into tokenized text format, where one token is one single word using tidytext.\n\n\nShow code\n\nhe_tokens <- he_df %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"[0-9]+|[[:punct:]]|\\\\(.*\\\\)\", replacement = \"\")) %>% # first get rid of numbers and citations because we are only analyzing the text\n  # then remove Fig and Table labels and other artifacts\n  mutate(text_full = gsub(x = text_full, pattern = \"figure\", replacement = \"\")) %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"figures\", replacement = \"\")) %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"table\", replacement = \"\")) %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"tables\", replacement = \"\")) %>% \n  mutate(text_full = gsub(x = text_full, pattern = \"Phil. Trans. R. Soc. B\", replacement = \"\")) %>%\n  mutate(text_full = gsub(x = text_full, pattern = \"royalsocietypublishing.org/journal/rstb\", replacement = \"\")) %>%\n  mutate(text_full = gsub(x = text_full, pattern = \"royalsocietypublishingorgjournalrstb\", replacement = \"\")) %>%\n  \n  unnest_tokens(word, text_full) %>% # from tidytext\n  select(-he_text) # get rid of first column that holds no new information\n\n\n\nNow remove stop words (i.e. common words like “the”, “is”, and “it”)\n\n\nShow code\n\nhe_nonstop_words <- he_tokens %>% \n  anti_join(stop_words) \n\n\n# count them by section (this is equivalent to group_by + summarize)\nnonstop_counts <- he_nonstop_words %>% \n  count(section, word)\n\n# find the top 10 words by section\ntop_10_words <- nonstop_counts %>% \n  group_by(section) %>% \n  arrange(-n) %>% \n  slice(1:10) # keep top ten\n\ntop_10_words\n\n\n# A tibble: 40 x 3\n# Groups:   section [4]\n   section    word           n\n   <chr>      <chr>      <int>\n 1 Conclusion carbon        40\n 2 Conclusion herbivores    26\n 3 Conclusion effects       21\n 4 Conclusion coastal       20\n 5 Conclusion cycle         18\n 6 Conclusion processes     15\n 7 Conclusion wetlands      15\n 8 Conclusion consumers     14\n 9 Conclusion soil          13\n10 Conclusion herbivore     12\n# … with 30 more rows\n\nVisualize results\nVisualize the top words\n\n\nShow code\n\nggplot(data = top_10_words, aes(x = reorder(word, n), y = n)) +\n  geom_col(fill = \"#597D35\") +\n  facet_wrap(~section, scales = \"free\") + # need scales = \"free\" to make it so axes (incl. x axis) is not the same in each plot\n  coord_flip() +\n  ylab(\"Word\") +\n  xlab(\"Number of times used\") +\n  theme_minimal()\n\n\n\n\nMake word clouds of the top 50 words in each section\n\n\nShow code\n\nintro_top50 <- \n  nonstop_counts %>% \n  filter(section == \"Introduction\") %>% \n  arrange(-n) %>% \n  slice(1:50)\n\nintro_cloud_Sill <- ggplot(data = intro_top50, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n)) +\n  scale_size_area(max_size = 5.5) + \n  ggtitle('Silliman') + \n  theme_void()\n# see some words got cut off, but we'll leave those be for now\n\n\n\n\n\nShow code\n\n# can make one for methods\nmethods_top50 <- \n  nonstop_counts %>% \n  filter(section == \"Methods\") %>% \n  arrange(-n) %>% \n  slice(1:50)\n\nmeth_cloud_Sill <- methods_cloud <- ggplot(data = methods_top50, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n)) +\n  scale_colour_continuous(type = \"viridis\") +\n  scale_size_area(max_size = 5.5) +\n  ggtitle('Silliman') +\n  theme_void()\n\n\n# or one for results\nresults_top50 <- \n  nonstop_counts %>% \n  filter(section == \"Results\") %>% \n  arrange(-n) %>% \n  slice(1:50)\n\nres_cloud_Sill <- ggplot(data = results_top50, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n)) +\n  scale_colour_continuous(type = \"gradient\", low = \"grey\", high = \"coral\") +\n  scale_size_area(max_size = 5.5) +\n  ggtitle('Silliman') +\n  theme_void()\n\n\n\n# or for the conclusion\nconcl_top50 <- \n  nonstop_counts %>% \n  filter(section == \"Conclusion\") %>% \n  arrange(-n) %>% \n  slice(1:50)\n\nconcl_cloud_Sill <- ggplot(data = concl_top50, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n)) +\n  scale_colour_continuous(type = \"gradient\", low = \"orange\", high = \"darkorchid\") +\n  scale_size_area(max_size = 5.5) +\n  ggtitle('Silliman')  +\n  theme_void()\n\n\n\nPlot them together to compare:\n\n\nShow code\n\n(intro_cloud_Sill + intro_cloud_Der) + plot_annotation(\n  title = 'Introduction')\n\n\n\nShow code\n\n(meth_cloud_Sill + meth_cloud_Der) + plot_annotation(\n  title = 'Methods')\n\n\n\nShow code\n\n(res_cloud_Sill + res_cloud_Der) + plot_annotation(\n  title = 'Results')\n\n\n\nShow code\n\n(concl_cloud_Sill + concl_cloud_Der) + plot_annotation(\n  title = 'Conclusions')\n\n\n\n\nConduct a sentiment analysis\nStart with the afinn lexicon again that ranges from -5 to 5.\n\n\nShow code\n\nafinn_lex <- get_sentiments(\"afinn\")\n\n# join matching words\nhe_afinn <- he_nonstop_words %>% \n  inner_join(get_sentiments(\"afinn\")) \n\n# can get total counts\nafinn_counts <- he_afinn %>% \n  count(section, value) # see how positive and negative the values are for each section\n\n# or could get mean value\nafinn_means <- he_afinn %>% \n  group_by(section) %>% \n  summarize(mean_afinn = mean(value))\n\n\n\nVisualize results\n\n\nShow code\n\nafinn_Sill <- ggplot(data = afinn_means, aes(x = reorder(section, mean_afinn), y = as.numeric(mean_afinn))) +\n  geom_col(fill = \"#597D35\") +\n  ylab(\"Mean afinn value (-5 to 5)\") +\n  xlab(\"Paper section\") +\n  coord_flip() +\n  scale_y_continuous(limits = c(-1,1)) +\n  theme_bw() +\n  geom_vline(aes(xintercept = 0)) + \n  ggtitle(\"Silliman\")\n\n\n\nPlot them together for comparison\n\n\nShow code\n\nafinn_Sill / afinn_Der + plot_annotation(\n  title = \"Mean afinn rankings\")\n\n\n\n\nThen we use the NRC lexicon again.\n\n\nShow code\n\nhe_nrc <- he_nonstop_words %>% \n  inner_join(get_sentiments(\"nrc\")) # have repeated values when there are multiple sentiments for a word\n\nhe_nrc_counts <- he_nrc %>% \n  count(section, sentiment) # 10 sentiments total in nrc\n\nnrc_Sill <- he_nrc_counts %>% \n  ggplot(aes(x = reorder(sentiment, n), y = n)) +\n  geom_col(fill = \"#597D35\") +\n  facet_wrap(~section) +\n  coord_flip() +\n  xlab(\"Seniment\") +\n  ylab(\"Word count\") +\n  theme_minimal() +\n  ggtitle(\"Silliman\")\n\n\n\nPlot them together\n\n\nShow code\n\nnrc_Sill + plot_annotation(\n  title = \"NRC Sentiment analysis\"\n) \n\n\n\nShow code\n\nnrc_Der \n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-24-textanalysis/textanalysis_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-02-24T10:55:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-05-welcome/",
    "title": "Principal Components Analysis ",
    "description": "Example of how to create a quick biplot using global environmental variables.",
    "author": [
      {
        "name": "Julianna Renzi",
        "url": {}
      }
    ],
    "date": "2021-02-05",
    "categories": [],
    "contents": "\n\nContents\nThe data\nBiplot\nInterpretation\n\nThe data\nThis dataset was taken from @zander_venter’s kaggle.com page, titled “Environmental variables for world countries”, although all of the data originally came from remotely sensed data sets available on Google Earth Engine. The majority of values are means from countries taken at ~10km resolution. Accessibility to cities is the travel time to cities in minutes, cropland cover is a percentage of the country’s land, tree canopy cover is a percentage of land covered by trees that are taller than 5m, precipitation is in mm, and temperature is in degrees Celcuis. More information can be found here: https://www.kaggle.com/zanderventer/environmental-variables-for-world-countries/version/4. We’ll be using this dataset to see how these environmental variables are related to continents. To see the code I used in each section click on the Show code option.\n\n\nShow code\n\n# read in the dataset using a relative path within the project\nworld_env <- read_csv(here(\"_posts\", \"2021-02-05-welcome\", \"data\", \"world_env_vars.csv\")) %>% \n  select(Country, accessibility_to_cities, cropland_cover,\n         tree_canopy_cover, rain_mean_annual, temp_annual_range,\n         temp_mean_annual, cloudiness) %>% # select the environmental variables we're interested in\n  drop_na() %>%  # see some countries (e.g. Antartica) don't have values for variables, so drop those\n  mutate(continent = countrycode(sourcevar = Country,\n                                 origin = \"country.name\",\n                                 destination = \"continent\")) %>% # add continent names\n  drop_na(continent) # see that some countries are misspelled so we drop those\n\n# make PCA:\nenv_pca <- world_env %>%   \n  select(-Country, -continent) %>% # can't have character columns\n  scale() %>% # want to scale variables so none will be overly weighted\n  prcomp() # run PCA--now it's not a df it's a list with information on the PCA\n\n\n\nBiplot\nNow we’ll plot the results as a biplot.\n\n\nShow code\n\nautoplot(env_pca,\n         data = world_env,\n         colour = \"continent\",\n         loadings = TRUE,\n         loadings.label = TRUE) +\n  scale_color_manual(values = inauguration(\"inauguration_2021\")) + # colors to match the inaugural ladies of the 2021 presidential inauguration\n  theme_minimal() + \n  theme(legend.position=\"top\", plot.caption = element_text(hjust = 0)) +\n  labs(caption = \"Figure 1: Biplot of the first two principal components of the PCA\")\n\n\n\n\nInterpretation\nAbout 70% of our data was explained in the first two principal components (PC1 and PC2, shown above), which suggests that we’re capturing a good amount of information in just these two dimensions, although keep in mind that ~30% of the variation between continents is still unaccounted for. Europe appeared to cluster together for the most part, but overlapped some with Asia, which was relatively diffuse in PCA-space. Africa and the Americas also overlapped considerably, suggesting they may be hard to classify based on these data alone. Oceana was relatively more distinct, but still overlapped with the Americas. Cropland cover and mean annual temperatures were strongly negatively correlated, perhaps because Europe has a high percentage of crop cover and much lower average temperatures (although perhaps a bigger range) than, say Oceana or Africa that tend to have milder winters and a lower percentage of crop cover. Accessibility to cities and annual mean temperatures were positively correlated, as were mean annual precipitation, tree canopy cover, and cloudiness.\n\n\n\n",
    "preview": "posts/2021-02-05-welcome/welcome_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-24T10:58:06-05:00",
    "input_file": {}
  }
]
